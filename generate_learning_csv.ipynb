{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jsonl = pd.read_json('data/train.jsonl', lines=True).set_index('id')\n",
    "dev_jsonl = pd.read_json('data/dev.jsonl', lines=True).set_index('id')\n",
    "test_jsonl = pd.read_json('data/test.jsonl', lines=True).set_index('id')\n",
    "\n",
    "train_jsonl['split'] = 'train'\n",
    "dev_jsonl['split'] = 'dev'\n",
    "test_jsonl['split'] = 'test'\n",
    "\n",
    "merged_df = pd.concat([train_jsonl, dev_jsonl, test_jsonl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vectors_folder = Path(\"info\") / \"scene\"\n",
    "image_vectors = dict()\n",
    "for image_vector_file in image_vectors_folder.iterdir():\n",
    "    image_id = int(image_vector_file.stem)\n",
    "    with image_vector_file.open() as f:\n",
    "        vector = np.array([float(scalar) for scalar in f.read().split('\\n') if scalar != ''])\n",
    "    image_vectors[image_id] = vector\n",
    "    \n",
    "merged_df['image_vector'] = merged_df.index.map(image_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "splitted = [nltk.word_tokenize(doc) for doc in merged_df['text']]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [[lemmatizer.lemmatize(word) for word in doc] for doc in splitted]\n",
    "tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(lemmatized)]\n",
    "doc2vec = Doc2Vec(tagged_docs)\n",
    "\n",
    "text_vectors = np.array([doc2vec.infer_vector(doc) for doc in lemmatized]\n",
    "merged_df['text_vector'] = text_vectors\n",
    "merged_df['image_vector'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_df = pd.DataFrame(merged_df, columns=['text_vector', 'image_vector', 'label', 'split'])\n",
    "learning_df['label'] = learning_df['label'].astype(bool)\n",
    "learning_df.to_pickle(\"learning_df.pickle\", protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "42953    [9.243041745321534e-07, 0.0010814660927280784,...\n",
       "23058    [4.7280764192692e-06, 0.0002732583088800311, 5...\n",
       "13894    [1.44773900956352e-06, 1.3311026123119518e-05,...\n",
       "37408    [2.800878974085208e-06, 3.871678563882597e-05,...\n",
       "82403    [1.6485433889101841e-06, 0.0007236988167278469...\n",
       "                               ...                        \n",
       "3869     [3.028519017789222e-07, 6.190018211782444e-06,...\n",
       "23817    [4.958599575388689e-08, 6.375514089995704e-07,...\n",
       "56280    [6.673790630884469e-05, 0.006667210254818201, ...\n",
       "29384    [2.6612129659042694e-05, 0.0003297853982076049...\n",
       "34127    [1.7552205235915608e-06, 0.0005712925922125578...\n",
       "Name: image_vector, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_df['image_vector']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
